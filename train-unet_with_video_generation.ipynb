{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-30T23:18:37.128114Z","iopub.execute_input":"2023-07-30T23:18:37.128535Z","iopub.status.idle":"2023-07-30T23:18:37.144881Z","shell.execute_reply.started":"2023-07-30T23:18:37.128502Z","shell.execute_reply":"2023-07-30T23:18:37.143720Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"/kaggle/input/sam-vit/sam_vit_b_01ec64.pth\n/kaggle/input/segment-anything/pytorch/vit-l/1/model.pth\n/kaggle/input/segment-anything/pytorch/vit-h/1/model.pth\n/kaggle/input/player-segmentation/Test.mp4\n/kaggle/input/player-segmentation/0_500/images/208.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q imantics\n!pip install -q segmentation_models_pytorch","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:37.147470Z","iopub.execute_input":"2023-07-30T23:18:37.148125Z","iopub.status.idle":"2023-07-30T23:18:43.326148Z","shell.execute_reply.started":"2023-07-30T23:18:37.148089Z","shell.execute_reply":"2023-07-30T23:18:43.324865Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"^C\nTraceback (most recent call last):\n  File \"/opt/conda/bin/pip\", line 6, in <module>\n    from pip._internal.cli.main import main\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n    from pip._internal.cli.autocompletion import autocomplete\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n    from pip._internal.cli.main_parser import create_main_parser\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n    from pip._internal.build_env import get_runnable_pip\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/build_env.py\", line 21, in <module>\n    from pip._internal.metadata import get_default_environment, get_environment\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/metadata/__init__.py\", line 9, in <module>\n    from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/metadata/base.py\", line 101, in <module>\n    class RequiresEntry(NamedTuple):\n  File \"/opt/conda/lib/python3.10/typing.py\", line 2279, in __new__\n    nm_tpl = _make_nmtuple(typename, types.items(),\n  File \"/opt/conda/lib/python3.10/typing.py\", line 2251, in _make_nmtuple\n    nm_tpl = collections.namedtuple(name, fields,\n  File \"/opt/conda/lib/python3.10/collections/__init__.py\", line 414, in namedtuple\n    __new__ = eval(code, namespace)\n  File \"<string>\", line 1, in <module>\nKeyboardInterrupt\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport json\n\nfrom tqdm import tqdm\nimport imantics\nimport numpy as np\nimport math\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.optim import Adam\nfrom sklearn.model_selection import train_test_split\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\nimport albumentations as A\nimport sys\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.328360Z","iopub.execute_input":"2023-07-30T23:18:43.328769Z","iopub.status.idle":"2023-07-30T23:18:43.339286Z","shell.execute_reply.started":"2023-07-30T23:18:43.328725Z","shell.execute_reply":"2023-07-30T23:18:43.338042Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### Extract Frames From Video File","metadata":{}},{"cell_type":"code","source":"input_path = \"../input/player-segmentation/\"\nframe_save_path = \"/kaggle/working/frames\"\n\nSTRIDE = 0.1\nMAX_IMAGE_SIZE = 1024","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.343117Z","iopub.execute_input":"2023-07-30T23:18:43.343437Z","iopub.status.idle":"2023-07-30T23:18:43.350156Z","shell.execute_reply.started":"2023-07-30T23:18:43.343401Z","shell.execute_reply":"2023-07-30T23:18:43.349221Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def get_frames_from_video(video_file, stride=1.0):\n    \"\"\"\n    video_file - path to file\n    stride - i.e 1.0 - extract frame every second, 0.5 - extract every 0.5 seconds\n    return: list of images, list of frame times in seconds\n    \"\"\"\n    video = cv2.VideoCapture(video_file)\n    fps = video.get(cv2.CAP_PROP_FPS)\n    i = 0.\n    images = []\n    frame_times = []\n\n    while video.isOpened():\n        ret, frame = video.read()\n        if ret:\n            images.append(frame)\n            frame_times.append(i)\n            i += stride\n            video.set(1, round(i * fps))\n        else:\n            video.release()\n            break\n    return images, frame_times\n\n\ndef resize_if_necessary(image, max_size=1024):\n    \"\"\"\n    if any spatial shape of image is greater \n    than max_size, resize image such that max. spatial shape = max_size,\n    otherwise return original image\n    \"\"\"\n    if max_size is None:\n        return image\n    height, width = image.shape[:2]\n    if max([height, width]) > max_size:\n        ratio = float(max_size / max([height, width]))\n        image = cv2.resize(image, (0, 0), fx=ratio, fy=ratio, interpolation=cv2.INTER_CUBIC)\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.351832Z","iopub.execute_input":"2023-07-30T23:18:43.352236Z","iopub.status.idle":"2023-07-30T23:18:43.364689Z","shell.execute_reply.started":"2023-07-30T23:18:43.352203Z","shell.execute_reply":"2023-07-30T23:18:43.363724Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"import math \nplt.figure(figsize=(50,30))\ncolumns = 5\nfor i, (image, frame_time) in enumerate(zip(images, frame_times)):\n    plt.subplot(math.ceil(len(images) / columns + 1), columns, i + 1).set_title(\"Frame time: \" + str(frame_time))\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.366483Z","iopub.execute_input":"2023-07-30T23:18:43.367032Z","iopub.status.idle":"2023-07-30T23:18:43.486610Z","shell.execute_reply.started":"2023-07-30T23:18:43.366988Z","shell.execute_reply":"2023-07-30T23:18:43.482588Z"},"trusted":true},"execution_count":55,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[55], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m30\u001b[39m))\n\u001b[1;32m      3\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (image, frame_time) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, \u001b[43mframe_times\u001b[49m)):\n\u001b[1;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m/\u001b[39m columns \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), columns, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame time: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(frame_time))\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n","\u001b[0;31mNameError\u001b[0m: name 'frame_times' is not defined"],"ename":"NameError","evalue":"name 'frame_times' is not defined","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 5000x3000 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"mkdir /kaggle/working/frames","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.487663Z","iopub.status.idle":"2023-07-30T23:18:43.488720Z","shell.execute_reply.started":"2023-07-30T23:18:43.488406Z","shell.execute_reply":"2023-07-30T23:18:43.488435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nSTRIDE = 0.1\nMAX_IMAGE_SIZE = 1024\n\ndef get_frames_from_video(video_file, stride=1.0):\n    \"\"\"\n    video_file - path to file\n    stride - i.e 1.0 - extract frame every second, 0.5 - extract every 0.5 seconds\n    return: list of images, list of frame times in seconds\n    \"\"\"\n    video = cv2.VideoCapture(video_file)\n    fps = video.get(cv2.CAP_PROP_FPS)\n    i = 0.\n    images = []\n    frame_times = []\n\n    while video.isOpened():\n        ret, frame = video.read()\n        if ret:\n            images.append(frame)\n            frame_times.append(i)\n            i += stride\n            video.set(1, round(i * fps))\n        else:\n            video.release()\n            break\n    return images, frame_times\n\n\ndef resize_if_necessary(image, max_size=1024):\n    \"\"\"\n    if any spatial shape of image is greater \n    than max_size, resize image such that max. spatial shape = max_size,\n    otherwise return original image\n    \"\"\"\n    if max_size is None:\n        return image\n    height, width = image.shape[:2]\n    if max([height, width]) > max_size:\n        ratio = float(max_size / max([height, width]))\n        image = cv2.resize(image, (0, 0), fx=ratio, fy=ratio, interpolation=cv2.INTER_CUBIC)\n    return image\n\n\ndef demo_generate_frames():\n    sample_video = '../input/player-segmentation/Test.mp4'\n    images, frame_times = get_frames_from_video(sample_video, STRIDE)\n    images = [resize_if_necessary(image, MAX_IMAGE_SIZE) for image in images]\n\n    return images\n\n\ndef generate_frames(input_path, save_path, stride, max_image_size):\n\n    images, frame_times = get_frames_from_video(input_path, stride)\n    images = [resize_if_necessary(image, max_image_size) for image in images]\n    for image, frame_time in zip(images, frame_times):\n        image_name = str(round(frame_time, 3)).replace(\".\", \"_\")\n        ssave_path = os.path.join(save_path, \"{}.jpg\".format(image_name))\n        print('saving:  ', ssave_path)\n        cv2.imwrite(ssave_path, image)  \n\n\nsample_video = '../input/player-segmentation/Test.mp4'\nframe_save_path = \"/kaggle/working/frames\"\n\ngenerate_frames(sample_video, frame_save_path, STRIDE, MAX_IMAGE_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.490593Z","iopub.status.idle":"2023-07-30T23:18:43.491374Z","shell.execute_reply.started":"2023-07-30T23:18:43.491115Z","shell.execute_reply":"2023-07-30T23:18:43.491138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recreate video from frames","metadata":{}},{"cell_type":"code","source":"# importing libraries\nimport os\nimport cv2 \nfrom PIL import Image \n  \n# Video Generating function\ndef generate_video(input_folder = '/kaggle/working/frames_out2', video_name = '_outvideo_unet.avi'):\n    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), 10, (512, 512)) \n  \n    # Appending the images to the video one by one\n    # for image in images: \n    #     video.write(cv2.imread(os.path.join(image_folder, image))) \n    \n    for sec in range(34):\n        for frame in range(10):\n            img = cv2.imread(f'/kaggle/working/frames_out2/{sec}_{frame}_out.jpg')\n            video.write(img)\n      \n    # Deallocating memories taken for window creation\n    # cv2.destroyAllWindows() \n    video.release()  # releasing the video generated\n  \n  \n# Calling the generate_video function\ngenerate_video()","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:51:55.932613Z","iopub.execute_input":"2023-07-30T23:51:55.933539Z","iopub.status.idle":"2023-07-30T23:51:56.793223Z","shell.execute_reply.started":"2023-07-30T23:51:55.933492Z","shell.execute_reply":"2023-07-30T23:51:56.792178Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"### Prepare data","metadata":{}},{"cell_type":"code","source":"N_IMAGES = 512\nTRAIN_IMAGE_SIZE = 512\nINPUT_IMAGE_SIZE = (1920, 1080)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.495059Z","iopub.status.idle":"2023-07-30T23:18:43.496196Z","shell.execute_reply.started":"2023-07-30T23:18:43.495927Z","shell.execute_reply":"2023-07-30T23:18:43.495954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/player-segmentation/instances_default.json\") as f:\n    annotations = json.load(f)\n\nmap_id_filename = {}\nfor index in range(len(annotations[\"images\"])):\n    map_id_filename[annotations[\"images\"][index][\"id\"]] = annotations[\"images\"][index][\"file_name\"]","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.497361Z","iopub.status.idle":"2023-07-30T23:18:43.498484Z","shell.execute_reply.started":"2023-07-30T23:18:43.498217Z","shell.execute_reply":"2023-07-30T23:18:43.498242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = np.zeros((N_IMAGES, TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE, 3), dtype=np.uint8)\nfor image_id, image_filename in map_id_filename.items():\n    cur_image = cv2.imread(f\"../input/player-segmentation/0_500/images/{image_filename}\")\n    cur_image = cv2.cvtColor(cur_image, cv2.COLOR_BGR2RGB)\n    cur_image = cv2.resize(cur_image, (TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE))\n\n    images[image_id - 1] = cur_image","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.499782Z","iopub.status.idle":"2023-07-30T23:18:43.500635Z","shell.execute_reply.started":"2023-07-30T23:18:43.500374Z","shell.execute_reply":"2023-07-30T23:18:43.500398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks = np.zeros((N_IMAGES, TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE), dtype=bool)\n\nfor index in range(len(annotations[\"annotations\"])):\n    image_id = annotations[\"annotations\"][index][\"image_id\"]\n    segmentation = annotations[\"annotations\"][index][\"segmentation\"]\n\n    cur_mask = imantics.Polygons(segmentation).mask(*INPUT_IMAGE_SIZE).array\n    cur_mask = cv2.resize(cur_mask.astype(float), (TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE)) >= 0.5\n\n    masks[image_id - 1] = masks[image_id - 1] | cur_mask","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.502147Z","iopub.status.idle":"2023-07-30T23:18:43.502988Z","shell.execute_reply.started":"2023-07-30T23:18:43.502746Z","shell.execute_reply":"2023-07-30T23:18:43.502769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(images[100])\nplt.imshow(masks[100], alpha=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.504518Z","iopub.status.idle":"2023-07-30T23:18:43.505556Z","shell.execute_reply.started":"2023-07-30T23:18:43.505272Z","shell.execute_reply":"2023-07-30T23:18:43.505295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    train_size = 0.8\n    batch_size = 4\n    lr = 0.001\n    n_epochs = 7\n    device = \"cuda\"\n    \ndef seed_everything(seed: int) -> None:\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(CFG.seed)\n\nindexes = list(range(N_IMAGES))\ntrain_indexes = indexes[: int(N_IMAGES * CFG.train_size)]\nvalid_indexes = indexes[int(N_IMAGES * CFG.train_size) :]","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.507267Z","iopub.status.idle":"2023-07-30T23:18:43.507824Z","shell.execute_reply.started":"2023-07-30T23:18:43.507518Z","shell.execute_reply":"2023-07-30T23:18:43.507542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms():\n    return A.Compose(\n        [\n            A.HueSaturationValue( # Change colors\n                p=1.0, \n                hue_shift_limit=(-20, 20), \n                sat_shift_limit=(-30, 30), \n                val_shift_limit=(-20, 20),\n            ),\n            A.HorizontalFlip(p=0.5),\n        ], \n        p=1.0\n    )","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.510579Z","iopub.status.idle":"2023-07-30T23:18:43.511702Z","shell.execute_reply.started":"2023-07-30T23:18:43.511502Z","shell.execute_reply":"2023-07-30T23:18:43.511520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, indexes, transform=None, preprocess=None):\n        self.indexes = indexes\n        self.transform = transform\n        self.preprocess = preprocess\n\n    def __len__(self):\n        return len(self.indexes)\n\n    def __getitem__(self, index):\n        _index = self.indexes[index]\n\n        image = images[_index]\n        mask = masks[_index]\n        \n        if self.transform:\n            data = {\"image\": image, \"mask\": mask}\n            augmented = self.transform(**data)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        \n        if self.preprocess:\n            image = self.preprocess(image)\n        \n        image = torch.tensor(image, dtype=torch.float)\n        mask = torch.tensor(mask, dtype=torch.float)\n\n        image = image.permute(2, 0, 1)\n        mask = mask.unsqueeze(0)\n\n        return {\"image\": image, \"mask\": mask}","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.512787Z","iopub.status.idle":"2023-07-30T23:18:43.513139Z","shell.execute_reply.started":"2023-07-30T23:18:43.512967Z","shell.execute_reply":"2023-07-30T23:18:43.512984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unet","metadata":{}},{"cell_type":"code","source":"model = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.515905Z","iopub.status.idle":"2023-07-30T23:18:43.516844Z","shell.execute_reply.started":"2023-07-30T23:18:43.516531Z","shell.execute_reply":"2023-07-30T23:18:43.516558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_input = get_preprocessing_fn(\"resnet34\", pretrained=\"imagenet\")","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.518292Z","iopub.status.idle":"2023-07-30T23:18:43.519128Z","shell.execute_reply.started":"2023-07-30T23:18:43.518851Z","shell.execute_reply":"2023-07-30T23:18:43.518875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(\n    train_indexes, transform=get_transforms(), preprocess=preprocess_input\n)\ntrain_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n\nvalid_dataset = CustomDataset(valid_indexes, preprocess=preprocess_input)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.520654Z","iopub.status.idle":"2023-07-30T23:18:43.521531Z","shell.execute_reply.started":"2023-07-30T23:18:43.521247Z","shell.execute_reply":"2023-07-30T23:18:43.521273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(CFG.device)\n\ncriterion = BCEWithLogitsLoss()\noptimizer = Adam(model.parameters(), lr=CFG.lr)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.523020Z","iopub.status.idle":"2023-07-30T23:18:43.523835Z","shell.execute_reply.started":"2023-07-30T23:18:43.523572Z","shell.execute_reply":"2023-07-30T23:18:43.523595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iou(outputs, labels) -> float:\n    intersection = np.sum(np.logical_and(outputs, labels), axis=(1, 2, 3))\n    union = np.sum(np.logical_or(outputs, labels), axis=(1, 2, 3))\n    iou = intersection / union\n    return np.mean(iou)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.525389Z","iopub.status.idle":"2023-07-30T23:18:43.526219Z","shell.execute_reply.started":"2023-07-30T23:18:43.525979Z","shell.execute_reply":"2023-07-30T23:18:43.526003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_history = []\nval_loss_history = []\ntrain_iou_history = []\nval_iou_history = []\n\nfor epoch in range(CFG.n_epochs):\n    train_loss = 0\n    train_iou = 0\n    model.train()\n    for i, batch in tqdm(enumerate(train_dataloader)):\n        inputs = batch[\"image\"].to(CFG.device)\n        labels = batch[\"mask\"].to(CFG.device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        train_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n        _iou = iou(outputs.detach().cpu().numpy() >= 0, labels.detach().cpu().numpy())\n        train_iou += _iou\n\n    train_loss /= len(train_dataloader)\n    train_iou /= len(train_dataloader)\n    train_loss_history.append(train_loss)\n    train_iou_history.append(train_iou)\n\n    val_loss = 0\n    val_iou = 0\n    model.eval()\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(valid_dataloader)):\n            inputs = batch[\"image\"].to(CFG.device)\n            labels = batch[\"mask\"].to(CFG.device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _iou = iou(outputs.detach().cpu().numpy() >= 0, labels.detach().cpu().numpy())\n            val_iou += _iou\n\n    val_loss /= len(valid_dataloader)\n    val_iou /= len(valid_dataloader)\n    val_loss_history.append(val_loss)\n    val_iou_history.append(val_iou)\n\n    print(\n        \"Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Train IOU: {:.4f}, Val IOU: {:.4f}\".format(\n            epoch + 1, CFG.n_epochs, train_loss, val_loss, train_iou, val_iou\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.527701Z","iopub.status.idle":"2023-07-30T23:18:43.528516Z","shell.execute_reply.started":"2023-07-30T23:18:43.528232Z","shell.execute_reply":"2023-07-30T23:18:43.528258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.plot(range(CFG.n_epochs), train_iou_history, label=\"train\")\nplt.plot(range(CFG.n_epochs), val_iou_history, label=\"valid\")\n\nplt.title(\"Train and Valid Score\", fontsize=16)\nplt.legend(fontsize=15)\nplt.ylabel(\"iou\", fontsize=14)\nplt.xlabel(\"epoch\", fontsize=14)\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.530101Z","iopub.status.idle":"2023-07-30T23:18:43.530931Z","shell.execute_reply.started":"2023-07-30T23:18:43.530663Z","shell.execute_reply":"2023-07-30T23:18:43.530704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    for i, batch in enumerate(valid_dataloader):\n        inputs = batch[\"image\"].to(CFG.device)\n        labels = batch[\"mask\"].to(CFG.device)\n\n        outputs = model(inputs)\n\n        np_labels = labels.detach().cpu().numpy()\n        np_outputs = outputs.detach().cpu().numpy()\n\n        for i in range(len(np_labels)):\n            plt.figure(figsize=(16, 6))\n            plt.subplot(1, 2, 1)\n            plt.imshow(np_labels[i][0])\n            plt.title(\"Target\")\n            plt.subplot(1, 2, 2)\n            plt.imshow(np_outputs[i][0] >= 0)\n            plt.title(\"Predict\")\n            plt.show()\n            \n        _iou = iou(np_outputs >= 0, np_labels)\n        val_iou += _iou\n        print(f'iou is {val_iou}')\n        \n        break\n        \nnp_outputs >= 0, np_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.532467Z","iopub.status.idle":"2023-07-30T23:18:43.533283Z","shell.execute_reply.started":"2023-07-30T23:18:43.533015Z","shell.execute_reply":"2023-07-30T23:18:43.533038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    for i, batch in enumerate(valid_dataloader):\n        inputs = batch[\"image\"].to(CFG.device)\n        labels = batch[\"mask\"].to(CFG.device)\n\n        outputs = model(inputs)\n\n        np_labels = labels.detach().cpu().numpy()\n        np_outputs = outputs.detach().cpu().numpy()\n\n        for i in range(len(np_labels)):\n            plt.figure(figsize=(16, 6))\n            plt.subplot(1, 2, 1)\n            plt.imshow(np_labels[i][0])\n            plt.title(\"Target\")\n            plt.subplot(1, 2, 2)\n            plt.imshow(np_outputs[i][0] >= 0)\n            plt.title(\"Predict\")\n            plt.show()\n            \n        _iou = iou(np_outputs >= 0, np_labels)\n        val_iou += _iou\n        print(f'iou is {val_iou}')\n        \n        break\n        \nnp_outputs >= 0, np_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.535034Z","iopub.status.idle":"2023-07-30T23:18:43.535886Z","shell.execute_reply.started":"2023-07-30T23:18:43.535619Z","shell.execute_reply":"2023-07-30T23:18:43.535642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ipywidgets as widgets\n\n\ndef show_anns(anns, axes=None):\n    if len(anns) == 0:\n        return\n    if axes:\n        ax = axes\n    else:\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.5)))\n#---------------------------------------------------       \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='red', marker='o', s=80, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='o', s=80, edgecolor='white', linewidth=1.25)  \n#---------------------------------------------------\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=1)\n    else:\n        color = np.array([200/255, 0/255, 0/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n#---------------------------------------------------   \ndef custom_plot(title,image,specific_point):\n    input_label = np.array([1])\n    #--------------\n    masks = mask_generator_2.generate(image)\n    \n    print('masks')\n    #--------------\n    predictor.set_image(image)\n    masks_p, scores, logits = predictor.predict(\n        point_coords=specific_point,\n        point_labels=input_label,\n        multimask_output=True,\n    )\n    return masks, masks_p, scores, logits\n    #--------------\n#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\n#     #fig.suptitle(f'{title} tumor')\n#     plt.axis('off')\n#     ax1.imshow(image)\n#     ax1.title.set_text(\"Image\")\n#     ax2.imshow(image)\n#     ax2.title.set_text(\"Image+Masks\")\n#     show_anns(masks, ax2)\n#     for i, (mask, score) in enumerate(zip(masks_p, scores)):\n#         if i==0:\n#             ax3.imshow(image)\n#             ax3.title.set_text(\"a specific object\")\n#             show_points(specific_point, input_label, ax3)\n#             show_mask(mask, ax4)\n#             ax4.title.set_text(f\"Mask - Score: {score:.3f}\")\n#     for ax in fig.get_axes():\n#         ax.label_outer()\n#         ax.axis('off')\n#-----------------\ndef draw(img_id):    \n    polygon = polygon_map[img_id]\n    img = cv2.imread(img_map[img_id])\n\n    blood_vessel = 0\n    glomerulus = 0\n    unsure = 0\n    annotations = []\n    for anno in polygon['annotations']:\n        if anno['type'] == 'blood_vessel':\n            color = (0,255,0)\n            blood_vessel += 1\n            \n        elif anno['type'] == 'glomerulus':\n            color = (0,0,0)\n            glomerulus += 1\n        else:\n            color = (255,0,0)\n            unsure += 1\n\n        pts = anno['coordinates']\n        pts = np.array(pts)\n        pts = pts.reshape(-1, 1, 2)\n        annotations.append(pts)\n        cv2.polylines(img, pts, True, color, 3)\n    \n    print(f'{blood_vessel = }')\n    print(f'{glomerulus = }')\n    print(f'{unsure = }')\n\n    plt.imshow(img)\n    return annotations\n\n#----------\ndef draw_sam(img_id, annotations):\n    image = cv2.cvtColor(\n        cv2.imread('/kaggle/input/hubmap-hacking-the-human-vasculature/train/' + img_id + '.tif'), \n        cv2.COLOR_BGR2RGB\n    )\n    specific_point = np.array([[50, 100]])\n    #custom_plot(img_id, image, specific_point)\n    (aa, a, b, c) = custom_plot(img_id, image, specific_point)\n    return (aa, a, b, c)\n    \noutput = widgets.Output()\n\n@output.capture()\ndef ipydisplay(change):\n    img_id = change['new']\n    ipd.clear_output()\n    print(\"Drawing....\")\n    annotations = draw(img_id)\n    print(\"Finished Drawing. SAM starting...\")\n#     print(annotations)\n    draw_sam(img_id, annotations)\n    plt.axis('off')\n    plt.show()\n    \n\n# you can only use this widget when actually running the notebook\n# select = widgets.Dropdown(options=list(polygon_map.keys()))\n# select.observe(ipydisplay, 'value')\n# widgets.VBox([select, output])","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.537361Z","iopub.status.idle":"2023-07-30T23:18:43.538272Z","shell.execute_reply.started":"2023-07-30T23:18:43.538000Z","shell.execute_reply":"2023-07-30T23:18:43.538026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst_images[2]","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:19:36.984046Z","iopub.execute_input":"2023-07-30T23:19:36.984410Z","iopub.status.idle":"2023-07-30T23:19:36.994238Z","shell.execute_reply.started":"2023-07-30T23:19:36.984381Z","shell.execute_reply":"2023-07-30T23:19:36.993201Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"array([[[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"mkdir /kaggle/working/frames_out2","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:46:03.567122Z","iopub.execute_input":"2023-07-30T23:46:03.567489Z","iopub.status.idle":"2023-07-30T23:46:04.671258Z","shell.execute_reply.started":"2023-07-30T23:46:03.567457Z","shell.execute_reply":"2023-07-30T23:46:04.669795Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"test_image_folder = '/kaggle/working/frames' # make sure to use your folder\ntest_images = [img for img in os.listdir(test_image_folder)\n              if img.endswith(\".jpg\") or\n                 img.endswith(\".jpeg\") or\n                 img.endswith(\"png\")]\n\ntst_images = np.zeros((len(test_images), TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE, 3), dtype=np.uint8) \nfor idx, image_filename in enumerate(test_images):\n    cur_image = cv2.imread(f\"/kaggle/working/frames/{image_filename}\")\n    cur_image = cv2.cvtColor(cur_image, cv2.COLOR_BGR2RGB)\n    cur_image = cv2.resize(cur_image, (TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE))\n    tst_images[idx] = cur_image\n\nclass CustomTstDataset(Dataset):\n    def __init__(self, indexes, transform=None, preprocess=None):\n        self.indexes = indexes\n        self.transform = transform\n        self.preprocess = preprocess\n\n    def __len__(self):\n        return len(self.indexes)\n\n    def __getitem__(self, index):\n        _index = self.indexes[index]\n\n        image = tst_images[_index]\n        \n        if self.transform:\n            data = {\"image\": image}\n            augmented = self.transform(**data)\n            image= augmented[\"image\"]\n        \n        if self.preprocess:\n            image = self.preprocess(image)\n        \n        image = torch.tensor(image, dtype=torch.float)\n        image = image.permute(2, 0, 1)\n\n        return {\"image\": image}\n\n\ntst_dataset = CustomTstDataset(tst_indices, preprocess=preprocess_input)\ntst_dataloader = DataLoader(tst_dataset, batch_size=1, shuffle=False)\n\npredict_save_path = \"/kaggle/working/frames_out2\"\n\nwith torch.no_grad():\n    for i, batch in enumerate(tst_dataloader):\n        inputs = batch[\"image\"].to(CFG.device)\n        outputs = model(inputs)\n        np_outputs = outputs.detach().cpu().numpy()\n        print(len(np_outputs), test_images[i])\n          \n        for j in range(len(outputs)):\n            image_name = test_images[i].split('.')[0]\n            spredict_save_path = os.path.join(predict_save_path, \"{}_out.jpg\".format(image_name))\n            print('saving:  ', spredict_save_path)\n            aa = 1.0 * (np_outputs[0][0] >= 0)\n            aa = (aa*255).astype(np.uint8)\n            cv2.imwrite(spredict_save_path, aa) \n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:49:20.471592Z","iopub.execute_input":"2023-07-30T23:49:20.472616Z","iopub.status.idle":"2023-07-30T23:49:32.629581Z","shell.execute_reply.started":"2023-07-30T23:49:20.472570Z","shell.execute_reply":"2023-07-30T23:49:32.628441Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"1 19_1.jpg\nsaving:   /kaggle/working/frames_out2/19_1_out.jpg\n1 13_2.jpg\nsaving:   /kaggle/working/frames_out2/13_2_out.jpg\n1 14_1.jpg\nsaving:   /kaggle/working/frames_out2/14_1_out.jpg\n1 5_0.jpg\nsaving:   /kaggle/working/frames_out2/5_0_out.jpg\n1 9_6.jpg\nsaving:   /kaggle/working/frames_out2/9_6_out.jpg\n1 7_4.jpg\nsaving:   /kaggle/working/frames_out2/7_4_out.jpg\n1 11_0.jpg\nsaving:   /kaggle/working/frames_out2/11_0_out.jpg\n1 15_0.jpg\nsaving:   /kaggle/working/frames_out2/15_0_out.jpg\n1 28_0.jpg\nsaving:   /kaggle/working/frames_out2/28_0_out.jpg\n1 24_2.jpg\nsaving:   /kaggle/working/frames_out2/24_2_out.jpg\n1 17_2.jpg\nsaving:   /kaggle/working/frames_out2/17_2_out.jpg\n1 32_3.jpg\nsaving:   /kaggle/working/frames_out2/32_3_out.jpg\n1 26_6.jpg\nsaving:   /kaggle/working/frames_out2/26_6_out.jpg\n1 26_5.jpg\nsaving:   /kaggle/working/frames_out2/26_5_out.jpg\n1 26_9.jpg\nsaving:   /kaggle/working/frames_out2/26_9_out.jpg\n1 6_5.jpg\nsaving:   /kaggle/working/frames_out2/6_5_out.jpg\n1 31_9.jpg\nsaving:   /kaggle/working/frames_out2/31_9_out.jpg\n1 10_2.jpg\nsaving:   /kaggle/working/frames_out2/10_2_out.jpg\n1 2_9.jpg\nsaving:   /kaggle/working/frames_out2/2_9_out.jpg\n1 33_5.jpg\nsaving:   /kaggle/working/frames_out2/33_5_out.jpg\n1 22_9.jpg\nsaving:   /kaggle/working/frames_out2/22_9_out.jpg\n1 4_5.jpg\nsaving:   /kaggle/working/frames_out2/4_5_out.jpg\n1 19_9.jpg\nsaving:   /kaggle/working/frames_out2/19_9_out.jpg\n1 15_2.jpg\nsaving:   /kaggle/working/frames_out2/15_2_out.jpg\n1 27_0.jpg\nsaving:   /kaggle/working/frames_out2/27_0_out.jpg\n1 32_2.jpg\nsaving:   /kaggle/working/frames_out2/32_2_out.jpg\n1 27_4.jpg\nsaving:   /kaggle/working/frames_out2/27_4_out.jpg\n1 3_0.jpg\nsaving:   /kaggle/working/frames_out2/3_0_out.jpg\n1 0_2.jpg\nsaving:   /kaggle/working/frames_out2/0_2_out.jpg\n1 20_4.jpg\nsaving:   /kaggle/working/frames_out2/20_4_out.jpg\n1 23_4.jpg\nsaving:   /kaggle/working/frames_out2/23_4_out.jpg\n1 2_1.jpg\nsaving:   /kaggle/working/frames_out2/2_1_out.jpg\n1 22_7.jpg\nsaving:   /kaggle/working/frames_out2/22_7_out.jpg\n1 25_4.jpg\nsaving:   /kaggle/working/frames_out2/25_4_out.jpg\n1 22_6.jpg\nsaving:   /kaggle/working/frames_out2/22_6_out.jpg\n1 8_4.jpg\nsaving:   /kaggle/working/frames_out2/8_4_out.jpg\n1 18_2.jpg\nsaving:   /kaggle/working/frames_out2/18_2_out.jpg\n1 5_9.jpg\nsaving:   /kaggle/working/frames_out2/5_9_out.jpg\n1 32_7.jpg\nsaving:   /kaggle/working/frames_out2/32_7_out.jpg\n1 22_1.jpg\nsaving:   /kaggle/working/frames_out2/22_1_out.jpg\n1 0_5.jpg\nsaving:   /kaggle/working/frames_out2/0_5_out.jpg\n1 22_3.jpg\nsaving:   /kaggle/working/frames_out2/22_3_out.jpg\n1 12_1.jpg\nsaving:   /kaggle/working/frames_out2/12_1_out.jpg\n1 10_6.jpg\nsaving:   /kaggle/working/frames_out2/10_6_out.jpg\n1 33_8.jpg\nsaving:   /kaggle/working/frames_out2/33_8_out.jpg\n1 31_4.jpg\nsaving:   /kaggle/working/frames_out2/31_4_out.jpg\n1 30_9.jpg\nsaving:   /kaggle/working/frames_out2/30_9_out.jpg\n1 4_8.jpg\nsaving:   /kaggle/working/frames_out2/4_8_out.jpg\n1 25_5.jpg\nsaving:   /kaggle/working/frames_out2/25_5_out.jpg\n1 21_6.jpg\nsaving:   /kaggle/working/frames_out2/21_6_out.jpg\n1 31_8.jpg\nsaving:   /kaggle/working/frames_out2/31_8_out.jpg\n1 9_5.jpg\nsaving:   /kaggle/working/frames_out2/9_5_out.jpg\n1 20_3.jpg\nsaving:   /kaggle/working/frames_out2/20_3_out.jpg\n1 19_5.jpg\nsaving:   /kaggle/working/frames_out2/19_5_out.jpg\n1 16_3.jpg\nsaving:   /kaggle/working/frames_out2/16_3_out.jpg\n1 18_0.jpg\nsaving:   /kaggle/working/frames_out2/18_0_out.jpg\n1 3_2.jpg\nsaving:   /kaggle/working/frames_out2/3_2_out.jpg\n1 15_6.jpg\nsaving:   /kaggle/working/frames_out2/15_6_out.jpg\n1 1_7.jpg\nsaving:   /kaggle/working/frames_out2/1_7_out.jpg\n1 33_9.jpg\nsaving:   /kaggle/working/frames_out2/33_9_out.jpg\n1 5_8.jpg\nsaving:   /kaggle/working/frames_out2/5_8_out.jpg\n1 30_7.jpg\nsaving:   /kaggle/working/frames_out2/30_7_out.jpg\n1 5_2.jpg\nsaving:   /kaggle/working/frames_out2/5_2_out.jpg\n1 24_4.jpg\nsaving:   /kaggle/working/frames_out2/24_4_out.jpg\n1 8_6.jpg\nsaving:   /kaggle/working/frames_out2/8_6_out.jpg\n1 12_0.jpg\nsaving:   /kaggle/working/frames_out2/12_0_out.jpg\n1 33_1.jpg\nsaving:   /kaggle/working/frames_out2/33_1_out.jpg\n1 9_4.jpg\nsaving:   /kaggle/working/frames_out2/9_4_out.jpg\n1 14_4.jpg\nsaving:   /kaggle/working/frames_out2/14_4_out.jpg\n1 23_6.jpg\nsaving:   /kaggle/working/frames_out2/23_6_out.jpg\n1 32_6.jpg\nsaving:   /kaggle/working/frames_out2/32_6_out.jpg\n1 28_8.jpg\nsaving:   /kaggle/working/frames_out2/28_8_out.jpg\n1 2_3.jpg\nsaving:   /kaggle/working/frames_out2/2_3_out.jpg\n1 26_8.jpg\nsaving:   /kaggle/working/frames_out2/26_8_out.jpg\n1 13_1.jpg\nsaving:   /kaggle/working/frames_out2/13_1_out.jpg\n1 16_4.jpg\nsaving:   /kaggle/working/frames_out2/16_4_out.jpg\n1 13_9.jpg\nsaving:   /kaggle/working/frames_out2/13_9_out.jpg\n1 15_8.jpg\nsaving:   /kaggle/working/frames_out2/15_8_out.jpg\n1 28_3.jpg\nsaving:   /kaggle/working/frames_out2/28_3_out.jpg\n1 7_0.jpg\nsaving:   /kaggle/working/frames_out2/7_0_out.jpg\n1 27_5.jpg\nsaving:   /kaggle/working/frames_out2/27_5_out.jpg\n1 18_4.jpg\nsaving:   /kaggle/working/frames_out2/18_4_out.jpg\n1 24_3.jpg\nsaving:   /kaggle/working/frames_out2/24_3_out.jpg\n1 16_0.jpg\nsaving:   /kaggle/working/frames_out2/16_0_out.jpg\n1 9_1.jpg\nsaving:   /kaggle/working/frames_out2/9_1_out.jpg\n1 33_0.jpg\nsaving:   /kaggle/working/frames_out2/33_0_out.jpg\n1 19_4.jpg\nsaving:   /kaggle/working/frames_out2/19_4_out.jpg\n1 22_8.jpg\nsaving:   /kaggle/working/frames_out2/22_8_out.jpg\n1 27_8.jpg\nsaving:   /kaggle/working/frames_out2/27_8_out.jpg\n1 21_1.jpg\nsaving:   /kaggle/working/frames_out2/21_1_out.jpg\n1 23_3.jpg\nsaving:   /kaggle/working/frames_out2/23_3_out.jpg\n1 15_9.jpg\nsaving:   /kaggle/working/frames_out2/15_9_out.jpg\n1 2_6.jpg\nsaving:   /kaggle/working/frames_out2/2_6_out.jpg\n1 16_2.jpg\nsaving:   /kaggle/working/frames_out2/16_2_out.jpg\n1 2_5.jpg\nsaving:   /kaggle/working/frames_out2/2_5_out.jpg\n1 10_4.jpg\nsaving:   /kaggle/working/frames_out2/10_4_out.jpg\n1 5_7.jpg\nsaving:   /kaggle/working/frames_out2/5_7_out.jpg\n1 30_3.jpg\nsaving:   /kaggle/working/frames_out2/30_3_out.jpg\n1 7_3.jpg\nsaving:   /kaggle/working/frames_out2/7_3_out.jpg\n1 12_2.jpg\nsaving:   /kaggle/working/frames_out2/12_2_out.jpg\n1 25_7.jpg\nsaving:   /kaggle/working/frames_out2/25_7_out.jpg\n1 29_1.jpg\nsaving:   /kaggle/working/frames_out2/29_1_out.jpg\n1 7_6.jpg\nsaving:   /kaggle/working/frames_out2/7_6_out.jpg\n1 27_6.jpg\nsaving:   /kaggle/working/frames_out2/27_6_out.jpg\n1 26_2.jpg\nsaving:   /kaggle/working/frames_out2/26_2_out.jpg\n1 18_5.jpg\nsaving:   /kaggle/working/frames_out2/18_5_out.jpg\n1 15_1.jpg\nsaving:   /kaggle/working/frames_out2/15_1_out.jpg\n1 4_4.jpg\nsaving:   /kaggle/working/frames_out2/4_4_out.jpg\n1 5_4.jpg\nsaving:   /kaggle/working/frames_out2/5_4_out.jpg\n1 28_4.jpg\nsaving:   /kaggle/working/frames_out2/28_4_out.jpg\n1 3_5.jpg\nsaving:   /kaggle/working/frames_out2/3_5_out.jpg\n1 18_3.jpg\nsaving:   /kaggle/working/frames_out2/18_3_out.jpg\n1 16_8.jpg\nsaving:   /kaggle/working/frames_out2/16_8_out.jpg\n1 7_9.jpg\nsaving:   /kaggle/working/frames_out2/7_9_out.jpg\n1 9_9.jpg\nsaving:   /kaggle/working/frames_out2/9_9_out.jpg\n1 0_9.jpg\nsaving:   /kaggle/working/frames_out2/0_9_out.jpg\n1 17_6.jpg\nsaving:   /kaggle/working/frames_out2/17_6_out.jpg\n1 0_3.jpg\nsaving:   /kaggle/working/frames_out2/0_3_out.jpg\n1 2_8.jpg\nsaving:   /kaggle/working/frames_out2/2_8_out.jpg\n1 17_9.jpg\nsaving:   /kaggle/working/frames_out2/17_9_out.jpg\n1 19_0.jpg\nsaving:   /kaggle/working/frames_out2/19_0_out.jpg\n1 2_2.jpg\nsaving:   /kaggle/working/frames_out2/2_2_out.jpg\n1 6_1.jpg\nsaving:   /kaggle/working/frames_out2/6_1_out.jpg\n1 19_8.jpg\nsaving:   /kaggle/working/frames_out2/19_8_out.jpg\n1 13_6.jpg\nsaving:   /kaggle/working/frames_out2/13_6_out.jpg\n1 20_5.jpg\nsaving:   /kaggle/working/frames_out2/20_5_out.jpg\n1 2_4.jpg\nsaving:   /kaggle/working/frames_out2/2_4_out.jpg\n1 31_7.jpg\nsaving:   /kaggle/working/frames_out2/31_7_out.jpg\n1 28_9.jpg\nsaving:   /kaggle/working/frames_out2/28_9_out.jpg\n1 10_7.jpg\nsaving:   /kaggle/working/frames_out2/10_7_out.jpg\n1 17_4.jpg\nsaving:   /kaggle/working/frames_out2/17_4_out.jpg\n1 15_3.jpg\nsaving:   /kaggle/working/frames_out2/15_3_out.jpg\n1 14_9.jpg\nsaving:   /kaggle/working/frames_out2/14_9_out.jpg\n1 11_3.jpg\nsaving:   /kaggle/working/frames_out2/11_3_out.jpg\n1 33_3.jpg\nsaving:   /kaggle/working/frames_out2/33_3_out.jpg\n1 10_5.jpg\nsaving:   /kaggle/working/frames_out2/10_5_out.jpg\n1 3_8.jpg\nsaving:   /kaggle/working/frames_out2/3_8_out.jpg\n1 14_2.jpg\nsaving:   /kaggle/working/frames_out2/14_2_out.jpg\n1 16_1.jpg\nsaving:   /kaggle/working/frames_out2/16_1_out.jpg\n1 12_8.jpg\nsaving:   /kaggle/working/frames_out2/12_8_out.jpg\n1 21_3.jpg\nsaving:   /kaggle/working/frames_out2/21_3_out.jpg\n1 1_4.jpg\nsaving:   /kaggle/working/frames_out2/1_4_out.jpg\n1 3_6.jpg\nsaving:   /kaggle/working/frames_out2/3_6_out.jpg\n1 12_6.jpg\nsaving:   /kaggle/working/frames_out2/12_6_out.jpg\n1 6_6.jpg\nsaving:   /kaggle/working/frames_out2/6_6_out.jpg\n1 23_5.jpg\nsaving:   /kaggle/working/frames_out2/23_5_out.jpg\n1 23_1.jpg\nsaving:   /kaggle/working/frames_out2/23_1_out.jpg\n1 14_7.jpg\nsaving:   /kaggle/working/frames_out2/14_7_out.jpg\n1 4_2.jpg\nsaving:   /kaggle/working/frames_out2/4_2_out.jpg\n1 9_2.jpg\nsaving:   /kaggle/working/frames_out2/9_2_out.jpg\n1 17_1.jpg\nsaving:   /kaggle/working/frames_out2/17_1_out.jpg\n1 33_6.jpg\nsaving:   /kaggle/working/frames_out2/33_6_out.jpg\n1 26_4.jpg\nsaving:   /kaggle/working/frames_out2/26_4_out.jpg\n1 7_7.jpg\nsaving:   /kaggle/working/frames_out2/7_7_out.jpg\n1 4_7.jpg\nsaving:   /kaggle/working/frames_out2/4_7_out.jpg\n1 27_7.jpg\nsaving:   /kaggle/working/frames_out2/27_7_out.jpg\n1 0_0.jpg\nsaving:   /kaggle/working/frames_out2/0_0_out.jpg\n1 32_1.jpg\nsaving:   /kaggle/working/frames_out2/32_1_out.jpg\n1 25_2.jpg\nsaving:   /kaggle/working/frames_out2/25_2_out.jpg\n1 14_6.jpg\nsaving:   /kaggle/working/frames_out2/14_6_out.jpg\n1 9_8.jpg\nsaving:   /kaggle/working/frames_out2/9_8_out.jpg\n1 12_4.jpg\nsaving:   /kaggle/working/frames_out2/12_4_out.jpg\n1 30_6.jpg\nsaving:   /kaggle/working/frames_out2/30_6_out.jpg\n1 8_1.jpg\nsaving:   /kaggle/working/frames_out2/8_1_out.jpg\n1 18_6.jpg\nsaving:   /kaggle/working/frames_out2/18_6_out.jpg\n1 13_8.jpg\nsaving:   /kaggle/working/frames_out2/13_8_out.jpg\n1 14_8.jpg\nsaving:   /kaggle/working/frames_out2/14_8_out.jpg\n1 19_7.jpg\nsaving:   /kaggle/working/frames_out2/19_7_out.jpg\n1 6_4.jpg\nsaving:   /kaggle/working/frames_out2/6_4_out.jpg\n1 32_5.jpg\nsaving:   /kaggle/working/frames_out2/32_5_out.jpg\n1 7_5.jpg\nsaving:   /kaggle/working/frames_out2/7_5_out.jpg\n1 2_7.jpg\nsaving:   /kaggle/working/frames_out2/2_7_out.jpg\n1 16_7.jpg\nsaving:   /kaggle/working/frames_out2/16_7_out.jpg\n1 10_9.jpg\nsaving:   /kaggle/working/frames_out2/10_9_out.jpg\n1 17_8.jpg\nsaving:   /kaggle/working/frames_out2/17_8_out.jpg\n1 26_7.jpg\nsaving:   /kaggle/working/frames_out2/26_7_out.jpg\n1 13_3.jpg\nsaving:   /kaggle/working/frames_out2/13_3_out.jpg\n1 29_3.jpg\nsaving:   /kaggle/working/frames_out2/29_3_out.jpg\n1 1_1.jpg\nsaving:   /kaggle/working/frames_out2/1_1_out.jpg\n1 15_4.jpg\nsaving:   /kaggle/working/frames_out2/15_4_out.jpg\n1 21_8.jpg\nsaving:   /kaggle/working/frames_out2/21_8_out.jpg\n1 21_9.jpg\nsaving:   /kaggle/working/frames_out2/21_9_out.jpg\n1 31_0.jpg\nsaving:   /kaggle/working/frames_out2/31_0_out.jpg\n1 8_7.jpg\nsaving:   /kaggle/working/frames_out2/8_7_out.jpg\n1 28_1.jpg\nsaving:   /kaggle/working/frames_out2/28_1_out.jpg\n1 21_5.jpg\nsaving:   /kaggle/working/frames_out2/21_5_out.jpg\n1 25_1.jpg\nsaving:   /kaggle/working/frames_out2/25_1_out.jpg\n1 8_2.jpg\nsaving:   /kaggle/working/frames_out2/8_2_out.jpg\n1 29_8.jpg\nsaving:   /kaggle/working/frames_out2/29_8_out.jpg\n1 27_3.jpg\nsaving:   /kaggle/working/frames_out2/27_3_out.jpg\n1 24_9.jpg\nsaving:   /kaggle/working/frames_out2/24_9_out.jpg\n1 10_3.jpg\nsaving:   /kaggle/working/frames_out2/10_3_out.jpg\n1 3_7.jpg\nsaving:   /kaggle/working/frames_out2/3_7_out.jpg\n1 0_7.jpg\nsaving:   /kaggle/working/frames_out2/0_7_out.jpg\n1 3_9.jpg\nsaving:   /kaggle/working/frames_out2/3_9_out.jpg\n1 0_4.jpg\nsaving:   /kaggle/working/frames_out2/0_4_out.jpg\n1 2_0.jpg\nsaving:   /kaggle/working/frames_out2/2_0_out.jpg\n1 25_8.jpg\nsaving:   /kaggle/working/frames_out2/25_8_out.jpg\n1 16_9.jpg\nsaving:   /kaggle/working/frames_out2/16_9_out.jpg\n1 25_3.jpg\nsaving:   /kaggle/working/frames_out2/25_3_out.jpg\n1 30_2.jpg\nsaving:   /kaggle/working/frames_out2/30_2_out.jpg\n1 0_8.jpg\nsaving:   /kaggle/working/frames_out2/0_8_out.jpg\n1 25_0.jpg\nsaving:   /kaggle/working/frames_out2/25_0_out.jpg\n1 29_0.jpg\nsaving:   /kaggle/working/frames_out2/29_0_out.jpg\n1 28_5.jpg\nsaving:   /kaggle/working/frames_out2/28_5_out.jpg\n1 29_4.jpg\nsaving:   /kaggle/working/frames_out2/29_4_out.jpg\n1 3_1.jpg\nsaving:   /kaggle/working/frames_out2/3_1_out.jpg\n1 29_7.jpg\nsaving:   /kaggle/working/frames_out2/29_7_out.jpg\n1 18_7.jpg\nsaving:   /kaggle/working/frames_out2/18_7_out.jpg\n1 5_1.jpg\nsaving:   /kaggle/working/frames_out2/5_1_out.jpg\n1 20_1.jpg\nsaving:   /kaggle/working/frames_out2/20_1_out.jpg\n1 1_9.jpg\nsaving:   /kaggle/working/frames_out2/1_9_out.jpg\n1 22_5.jpg\nsaving:   /kaggle/working/frames_out2/22_5_out.jpg\n1 5_6.jpg\nsaving:   /kaggle/working/frames_out2/5_6_out.jpg\n1 13_0.jpg\nsaving:   /kaggle/working/frames_out2/13_0_out.jpg\n1 1_8.jpg\nsaving:   /kaggle/working/frames_out2/1_8_out.jpg\n1 29_2.jpg\nsaving:   /kaggle/working/frames_out2/29_2_out.jpg\n1 26_1.jpg\nsaving:   /kaggle/working/frames_out2/26_1_out.jpg\n1 12_9.jpg\nsaving:   /kaggle/working/frames_out2/12_9_out.jpg\n1 32_4.jpg\nsaving:   /kaggle/working/frames_out2/32_4_out.jpg\n1 4_3.jpg\nsaving:   /kaggle/working/frames_out2/4_3_out.jpg\n1 21_0.jpg\nsaving:   /kaggle/working/frames_out2/21_0_out.jpg\n1 8_3.jpg\nsaving:   /kaggle/working/frames_out2/8_3_out.jpg\n1 17_5.jpg\nsaving:   /kaggle/working/frames_out2/17_5_out.jpg\n1 20_7.jpg\nsaving:   /kaggle/working/frames_out2/20_7_out.jpg\n1 8_8.jpg\nsaving:   /kaggle/working/frames_out2/8_8_out.jpg\n1 30_4.jpg\nsaving:   /kaggle/working/frames_out2/30_4_out.jpg\n1 1_6.jpg\nsaving:   /kaggle/working/frames_out2/1_6_out.jpg\n1 1_0.jpg\nsaving:   /kaggle/working/frames_out2/1_0_out.jpg\n1 4_0.jpg\nsaving:   /kaggle/working/frames_out2/4_0_out.jpg\n1 5_3.jpg\nsaving:   /kaggle/working/frames_out2/5_3_out.jpg\n1 20_6.jpg\nsaving:   /kaggle/working/frames_out2/20_6_out.jpg\n1 4_6.jpg\nsaving:   /kaggle/working/frames_out2/4_6_out.jpg\n1 21_7.jpg\nsaving:   /kaggle/working/frames_out2/21_7_out.jpg\n1 28_6.jpg\nsaving:   /kaggle/working/frames_out2/28_6_out.jpg\n1 10_1.jpg\nsaving:   /kaggle/working/frames_out2/10_1_out.jpg\n1 17_3.jpg\nsaving:   /kaggle/working/frames_out2/17_3_out.jpg\n1 11_2.jpg\nsaving:   /kaggle/working/frames_out2/11_2_out.jpg\n1 24_6.jpg\nsaving:   /kaggle/working/frames_out2/24_6_out.jpg\n1 0_6.jpg\nsaving:   /kaggle/working/frames_out2/0_6_out.jpg\n1 22_4.jpg\nsaving:   /kaggle/working/frames_out2/22_4_out.jpg\n1 29_5.jpg\nsaving:   /kaggle/working/frames_out2/29_5_out.jpg\n1 7_8.jpg\nsaving:   /kaggle/working/frames_out2/7_8_out.jpg\n1 19_6.jpg\nsaving:   /kaggle/working/frames_out2/19_6_out.jpg\n1 6_2.jpg\nsaving:   /kaggle/working/frames_out2/6_2_out.jpg\n1 1_2.jpg\nsaving:   /kaggle/working/frames_out2/1_2_out.jpg\n1 4_1.jpg\nsaving:   /kaggle/working/frames_out2/4_1_out.jpg\n1 11_7.jpg\nsaving:   /kaggle/working/frames_out2/11_7_out.jpg\n1 29_9.jpg\nsaving:   /kaggle/working/frames_out2/29_9_out.jpg\n1 0_1.jpg\nsaving:   /kaggle/working/frames_out2/0_1_out.jpg\n1 31_5.jpg\nsaving:   /kaggle/working/frames_out2/31_5_out.jpg\n1 18_9.jpg\nsaving:   /kaggle/working/frames_out2/18_9_out.jpg\n1 23_9.jpg\nsaving:   /kaggle/working/frames_out2/23_9_out.jpg\n1 14_5.jpg\nsaving:   /kaggle/working/frames_out2/14_5_out.jpg\n1 5_5.jpg\nsaving:   /kaggle/working/frames_out2/5_5_out.jpg\n1 7_1.jpg\nsaving:   /kaggle/working/frames_out2/7_1_out.jpg\n1 30_8.jpg\nsaving:   /kaggle/working/frames_out2/30_8_out.jpg\n1 9_3.jpg\nsaving:   /kaggle/working/frames_out2/9_3_out.jpg\n1 9_0.jpg\nsaving:   /kaggle/working/frames_out2/9_0_out.jpg\n1 18_8.jpg\nsaving:   /kaggle/working/frames_out2/18_8_out.jpg\n1 1_5.jpg\nsaving:   /kaggle/working/frames_out2/1_5_out.jpg\n1 10_0.jpg\nsaving:   /kaggle/working/frames_out2/10_0_out.jpg\n1 23_8.jpg\nsaving:   /kaggle/working/frames_out2/23_8_out.jpg\n1 32_8.jpg\nsaving:   /kaggle/working/frames_out2/32_8_out.jpg\n1 3_3.jpg\nsaving:   /kaggle/working/frames_out2/3_3_out.jpg\n1 7_2.jpg\nsaving:   /kaggle/working/frames_out2/7_2_out.jpg\n1 8_0.jpg\nsaving:   /kaggle/working/frames_out2/8_0_out.jpg\n1 32_9.jpg\nsaving:   /kaggle/working/frames_out2/32_9_out.jpg\n1 6_9.jpg\nsaving:   /kaggle/working/frames_out2/6_9_out.jpg\n1 13_7.jpg\nsaving:   /kaggle/working/frames_out2/13_7_out.jpg\n1 18_1.jpg\nsaving:   /kaggle/working/frames_out2/18_1_out.jpg\n1 24_8.jpg\nsaving:   /kaggle/working/frames_out2/24_8_out.jpg\n1 24_5.jpg\nsaving:   /kaggle/working/frames_out2/24_5_out.jpg\n1 10_8.jpg\nsaving:   /kaggle/working/frames_out2/10_8_out.jpg\n1 27_2.jpg\nsaving:   /kaggle/working/frames_out2/27_2_out.jpg\n1 14_0.jpg\nsaving:   /kaggle/working/frames_out2/14_0_out.jpg\n1 13_5.jpg\nsaving:   /kaggle/working/frames_out2/13_5_out.jpg\n1 33_7.jpg\nsaving:   /kaggle/working/frames_out2/33_7_out.jpg\n1 30_0.jpg\nsaving:   /kaggle/working/frames_out2/30_0_out.jpg\n1 23_2.jpg\nsaving:   /kaggle/working/frames_out2/23_2_out.jpg\n1 26_3.jpg\nsaving:   /kaggle/working/frames_out2/26_3_out.jpg\n1 11_8.jpg\nsaving:   /kaggle/working/frames_out2/11_8_out.jpg\n1 22_2.jpg\nsaving:   /kaggle/working/frames_out2/22_2_out.jpg\n1 16_6.jpg\nsaving:   /kaggle/working/frames_out2/16_6_out.jpg\n1 29_6.jpg\nsaving:   /kaggle/working/frames_out2/29_6_out.jpg\n1 19_2.jpg\nsaving:   /kaggle/working/frames_out2/19_2_out.jpg\n1 17_7.jpg\nsaving:   /kaggle/working/frames_out2/17_7_out.jpg\n1 6_7.jpg\nsaving:   /kaggle/working/frames_out2/6_7_out.jpg\n1 1_3.jpg\nsaving:   /kaggle/working/frames_out2/1_3_out.jpg\n1 28_7.jpg\nsaving:   /kaggle/working/frames_out2/28_7_out.jpg\n1 11_4.jpg\nsaving:   /kaggle/working/frames_out2/11_4_out.jpg\n1 12_5.jpg\nsaving:   /kaggle/working/frames_out2/12_5_out.jpg\n1 21_2.jpg\nsaving:   /kaggle/working/frames_out2/21_2_out.jpg\n1 27_1.jpg\nsaving:   /kaggle/working/frames_out2/27_1_out.jpg\n1 11_6.jpg\nsaving:   /kaggle/working/frames_out2/11_6_out.jpg\n1 20_8.jpg\nsaving:   /kaggle/working/frames_out2/20_8_out.jpg\n1 33_2.jpg\nsaving:   /kaggle/working/frames_out2/33_2_out.jpg\n1 11_5.jpg\nsaving:   /kaggle/working/frames_out2/11_5_out.jpg\n1 22_0.jpg\nsaving:   /kaggle/working/frames_out2/22_0_out.jpg\n1 31_1.jpg\nsaving:   /kaggle/working/frames_out2/31_1_out.jpg\n1 20_0.jpg\nsaving:   /kaggle/working/frames_out2/20_0_out.jpg\n1 15_7.jpg\nsaving:   /kaggle/working/frames_out2/15_7_out.jpg\n1 14_3.jpg\nsaving:   /kaggle/working/frames_out2/14_3_out.jpg\n1 30_5.jpg\nsaving:   /kaggle/working/frames_out2/30_5_out.jpg\n1 20_2.jpg\nsaving:   /kaggle/working/frames_out2/20_2_out.jpg\n1 17_0.jpg\nsaving:   /kaggle/working/frames_out2/17_0_out.jpg\n1 26_0.jpg\nsaving:   /kaggle/working/frames_out2/26_0_out.jpg\n1 15_5.jpg\nsaving:   /kaggle/working/frames_out2/15_5_out.jpg\n1 23_7.jpg\nsaving:   /kaggle/working/frames_out2/23_7_out.jpg\n1 30_1.jpg\nsaving:   /kaggle/working/frames_out2/30_1_out.jpg\n1 31_6.jpg\nsaving:   /kaggle/working/frames_out2/31_6_out.jpg\n1 19_3.jpg\nsaving:   /kaggle/working/frames_out2/19_3_out.jpg\n1 9_7.jpg\nsaving:   /kaggle/working/frames_out2/9_7_out.jpg\n1 16_5.jpg\nsaving:   /kaggle/working/frames_out2/16_5_out.jpg\n1 32_0.jpg\nsaving:   /kaggle/working/frames_out2/32_0_out.jpg\n1 11_9.jpg\nsaving:   /kaggle/working/frames_out2/11_9_out.jpg\n1 24_7.jpg\nsaving:   /kaggle/working/frames_out2/24_7_out.jpg\n1 12_7.jpg\nsaving:   /kaggle/working/frames_out2/12_7_out.jpg\n1 6_0.jpg\nsaving:   /kaggle/working/frames_out2/6_0_out.jpg\n1 20_9.jpg\nsaving:   /kaggle/working/frames_out2/20_9_out.jpg\n1 8_9.jpg\nsaving:   /kaggle/working/frames_out2/8_9_out.jpg\n1 27_9.jpg\nsaving:   /kaggle/working/frames_out2/27_9_out.jpg\n1 4_9.jpg\nsaving:   /kaggle/working/frames_out2/4_9_out.jpg\n1 25_6.jpg\nsaving:   /kaggle/working/frames_out2/25_6_out.jpg\n1 33_4.jpg\nsaving:   /kaggle/working/frames_out2/33_4_out.jpg\n1 11_1.jpg\nsaving:   /kaggle/working/frames_out2/11_1_out.jpg\n1 21_4.jpg\nsaving:   /kaggle/working/frames_out2/21_4_out.jpg\n1 25_9.jpg\nsaving:   /kaggle/working/frames_out2/25_9_out.jpg\n1 24_1.jpg\nsaving:   /kaggle/working/frames_out2/24_1_out.jpg\n1 23_0.jpg\nsaving:   /kaggle/working/frames_out2/23_0_out.jpg\n1 6_3.jpg\nsaving:   /kaggle/working/frames_out2/6_3_out.jpg\n1 24_0.jpg\nsaving:   /kaggle/working/frames_out2/24_0_out.jpg\n1 3_4.jpg\nsaving:   /kaggle/working/frames_out2/3_4_out.jpg\n1 28_2.jpg\nsaving:   /kaggle/working/frames_out2/28_2_out.jpg\n1 31_3.jpg\nsaving:   /kaggle/working/frames_out2/31_3_out.jpg\n1 8_5.jpg\nsaving:   /kaggle/working/frames_out2/8_5_out.jpg\n1 12_3.jpg\nsaving:   /kaggle/working/frames_out2/12_3_out.jpg\n1 31_2.jpg\nsaving:   /kaggle/working/frames_out2/31_2_out.jpg\n1 6_8.jpg\nsaving:   /kaggle/working/frames_out2/6_8_out.jpg\n1 13_4.jpg\nsaving:   /kaggle/working/frames_out2/13_4_out.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"tst_indices = list(range(len(test_images)))\nlen(tst_images)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.542115Z","iopub.status.idle":"2023-07-30T23:18:43.542928Z","shell.execute_reply.started":"2023-07-30T23:18:43.542670Z","shell.execute_reply":"2023-07-30T23:18:43.542710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_sam(img_id, annotations):\n    image = cv2.cvtColor(\n        cv2.imread('../input/player-segmentation/0_500/images/' + str(img_id) + '.jpg'), \n        cv2.COLOR_BGR2RGB\n    )\n    specific_point = np.array([[50, 100]])\n    return custom_plot(img_id, image, specific_point)\n    \n(masks, masks_p, scores, logits) = draw_sam(2, None)\n#masks_p, scores, logits","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.544392Z","iopub.status.idle":"2023-07-30T23:18:43.545289Z","shell.execute_reply.started":"2023-07-30T23:18:43.544996Z","shell.execute_reply":"2023-07-30T23:18:43.545023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.cvtColor(\n        cv2.imread('../input/player-segmentation/0_500/images/2.jpg'), \n        cv2.COLOR_BGR2RGB\n    )\nimage = cv2.resize(image, (256, 256))\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\nplt.axis('off')\n# ax1.imshow(image)\n# ax1.title.set_text(\"Image\")\nax2.imshow(image)\nax2.title.set_text(\"Image+Masks\")\nshow_anns(masks, ax2)\nspecific_point = np.array([[50, 100]])\n# for i, (mask, score) in enumerate(zip(masks_p, scores)):\n#     if i==0:\n#         ax3.imshow(image)\n#         ax3.title.set_text(\"a specific object\")\n#         show_points(specific_point, input_label, ax3)\n#         show_mask(mask, ax4)\n#         ax4.title.set_text(f\"Mask - Score: {score:.3f}\")\nfor ax in fig.get_axes():\n    ax.label_outer()\n    ax.axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-07-30T23:18:43.546836Z","iopub.status.idle":"2023-07-30T23:18:43.547622Z","shell.execute_reply.started":"2023-07-30T23:18:43.547362Z","shell.execute_reply":"2023-07-30T23:18:43.547385Z"},"trusted":true},"execution_count":null,"outputs":[]}]}